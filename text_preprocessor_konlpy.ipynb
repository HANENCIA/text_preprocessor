{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED: KoNLPy에서 추출된 한글 명사 리스트에서 2글자 미만으로 구성된 단어 삭제\n",
    "#\n",
    "# 1. Sample Data/Input Example\n",
    "# input_data = [['사과', '가', '어디']]\n",
    "# output_data = remove_below_two_char_in_noun_voca_list(input_data)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: [['사과, '어디']]\n",
    "\n",
    "def remove_below_two_char_in_noun_voca_list(noun_voca_list):\n",
    "    changed_noun_voca_list = []\n",
    "    for noun_voca_line in noun_voca_list:\n",
    "        changed_noun_voca_line = [noun_word for noun_word in noun_voca_line if len(noun_word) > 1]\n",
    "        changed_noun_voca_list.append(changed_noun_voca_line)\n",
    "    return changed_noun_voca_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED: KoNLPy에서 추출된 한글 명사 리스트에서 특정 단어 삭제\n",
    "# \n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바'], ['딸기', '산딸기']]\n",
    "# output_data = remove_text_from_noun_voca_list(input_data, '딸기')\n",
    "# \n",
    "# 2. Output Example/Data\n",
    "# 해당 글자가 포함된 단어가 아래와 같이 검출되었음\n",
    "# 산딸기\n",
    "# output_data: [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바'], ['산딸기']]\n",
    "\n",
    "def remove_text_from_noun_voca_list(noun_voca_list, text):\n",
    "    similar_noun_voca_list = []\n",
    "    changed_noun_voca_list = []\n",
    "    for noun_voca_line in noun_voca_list:\n",
    "        for noun_word in noun_voca_line:\n",
    "            if text in noun_word:\n",
    "                if noun_word in similar_noun_voca_list:\n",
    "                    pass\n",
    "                elif text != noun_word:\n",
    "                    similar_noun_voca_list.append(noun_word) \n",
    "            if text is noun_voca_line:\n",
    "                pass\n",
    "            else:\n",
    "                changed_noun_voca_list.append(noun_voca_line)\n",
    "    if len(similar_noun_voca_list)==0:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"해당 글자가 포함된 단어가 아래와 같이 검출되었음\")\n",
    "        print(\", \".join(similar_noun_voca_list))\n",
    "    return changed_noun_voca_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # DEPRECATED: KoNLPy에서 추출된 한글 명사 리스트에서 두 글자로 이루어진 단어만 추출 후 STOPWORDS 삭제\n",
    "#\n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바'], ['딸기', '산딸기']]\n",
    "# stopwords = ['딸기']\n",
    "# output_data = remove_stopwords(input_data, stopwords)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바'], ['산딸기']]\n",
    "\n",
    "def remove_stopwords(voca_list, stopword_list):\n",
    "    stopword_replaced_voca_list = []\n",
    "    sentence_list = make_sentence_list_from_voca_list(voca_list)\n",
    "    re_space = re.compile(r'\\s+')\n",
    "\n",
    "    for line in sentence_list:\n",
    "        for stopword in stopword_list:\n",
    "            line = ' '.join(re.compile('[가-힣]{2,}').findall(line))\n",
    "            re_word = re.compile(r'\\b{0}\\b'.format(stopword))\n",
    "            line = (re_space.sub(' ', re_word.sub('', line)).strip())\n",
    "        stopword_replaced_voca_list.append(line.split(' '))\n",
    "\n",
    "    return stopword_replaced_voca_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konlpy를 이용하여 명사 추출\n",
    "#\n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = ['구아바 구아바 망고를 유혹하네 딱 걸렸네, 포시즌을 낳았네', '젖소방은 없습니다.']\n",
    "# konlpy_opt = 'Hannanum', 'Kkma', 'Komoran', 'Mecab', 'Okt' 중 1개 입력\n",
    "# Konlpy tag Package: https://konlpy-ko.readthedocs.io/ko/latest/api/konlpy.tag/\n",
    "# input: output_data = make_noun_voca_list(input_data, konlpy_opt)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: [['구아바', '구아바', '망고', '유혹', '포', '시즌'], ['젖', '소방']]\n",
    "\n",
    "def make_noun_voca_list(raw_list, konlpy_opt):\n",
    "    noun_voca_list = []\n",
    "    assert konlpy_opt in ['Hannanum', 'Kkma', 'Komoran', 'Mecab', 'Okt']\n",
    "\n",
    "    if konlpy_opt == 'Hannanum':\n",
    "        nlp = Hannanum()\n",
    "    elif konlpy_opt == 'Kkma':\n",
    "        nlp = Kkma()\n",
    "    elif konlpy_opt == 'Komoran':\n",
    "        nlp = Komoran()\n",
    "    elif konlpy_opt == 'Mecab':\n",
    "        nlp = Mecab()\n",
    "    elif konlpy_opt == 'Okt':\n",
    "        nlp = Okt()\n",
    "\n",
    "    for line in raw_list:\n",
    "        if type(line).__name__ == 'float' or type(line).__name__ == 'int':\n",
    "            noun_voca_list.append('')\n",
    "        else:\n",
    "            noun_voca_list.append(nlp.nouns(' '.join(re.compile('[가-힣0-9]+').findall(line))))\n",
    "\n",
    "    return noun_voca_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 리스트를 띄어쓰기를 기준으로 join\n",
    "#\n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바']]\n",
    "# input: output_data = make_sentence_list_from_voca_list(input_data)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: ['사과 바나나 포도', '수박 파인애플 구아바']\n",
    "\n",
    "def make_sentence_list_from_voca_list(voca_list):\n",
    "    sentence_list = []\n",
    "    for voca_line in voca_list:\n",
    "        sentence_list.append(\" \".join(voca_line))\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 띄어쓰기를 기준으로 분할하여 list화\n",
    "#\n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = ['사과 바나나 포도', '수박 파인애플 구아바']\n",
    "# output_data = make_voca_list_from_sentence_list(input_data)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바']]\n",
    "\n",
    "def make_voca_list_from_sentence_list(sentence_list):\n",
    "    voca_list = []\n",
    "    for sentence_line in sentence_list:\n",
    "        voca_list.append(sentence_line.split(' '))\n",
    "    return voca_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# KoNLPy에서 추출된 한글 명사 리스트에서 두 글자로 이루어진 단어만 추출 후 STOPWORDS 삭제\n",
    "#\n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바'], ['딸기', '산딸기']]\n",
    "# stopwords = ['딸기']\n",
    "# output_data = remove_stopwords(input_data, stopwords)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: [['사과', '바나나', '포도'], ['수박', '파인애플', '구아바'], ['산딸기']]\n",
    "\n",
    "def remove_stopwords(voca_list, stopword_list):\n",
    "    stopword_replaced_voca_list = []\n",
    "\n",
    "    for line in voca_list:\n",
    "        stopword_replaced_voca_list.append(list(filter(lambda x: len(x) > 1 and x not in stopword_list, line)))\n",
    "\n",
    "    return stopword_replaced_voca_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도순 출력\n",
    "#\n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = [['사과', '바나나', '사과'], ['딸기', '사과', '바나나', '수박']]\n",
    "# top_count = n (int, 상위 n위)\n",
    "# print_most_common_words(input_data, 4)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: [('사과', 3), ('바나나', 2), ('딸기', 1), ('수박', 1)]\n",
    "# 주의: 동 순위가 있다면 앞에 있는 element 순서대로 끊어짐 (예: [('사과', 3), ('바나나', 2), ('딸기', 1)])\n",
    "\n",
    "def print_most_common_words(voca_list, top_count):\n",
    "    print(Counter(list(chain.from_iterable(voca_list))).most_common(top_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 단어 빈도 데이터 저장\n",
    "#\n",
    "# 1. Sample Data/Input Exmaple\n",
    "# input_data = [['사과', '바나나', '사과'], ['딸기', '사과', '바나나', '수박']]\n",
    "# top_count = n (int, 상위 n위, 저장 CSV 파일 경로)\n",
    "# print_most_common_words(input_data, 4)\n",
    "#\n",
    "# 2. Output Example/Data\n",
    "# output_data: CSV 파일\n",
    "# Column Name: WORD, COUNT\n",
    "# 주의: 동 순위가 있다면 앞에 있는 element 순서대로 끊어짐 (예: [('사과', 3), ('바나나', 2), ('딸기', 1)])\n",
    "\n",
    "def save_most_common_words(voca_list, top_count, output_path):\n",
    "    word_count_list = Counter(list(chain.from_iterable(voca_list))).most_common(top_count)\n",
    "    word_list = []\n",
    "    count_list = []\n",
    "    for idx in range(len(word_count_list)):\n",
    "        word_list.append(word_count_list[idx][0])\n",
    "        count_list.append(word_count_list[idx][1])\n",
    "    word_count_df = pd.DataFrame.from_records(zip(word_list, count_list), columns=['WORD', 'COUNT'])\n",
    "\n",
    "    word_count_df.to_csv(output_path, sep=',', index=False, encoding='utf-8-sig')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = 'data/sample_newspaper.csv'\n",
    "raw_list_name = '기사'\n",
    "\n",
    "raw_df = pd.read_csv(raw_path, sep=',', encoding='utf-8')\n",
    "raw_list = raw_df[raw_list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_noun_voca_list = make_noun_voca_list(raw_list, 'Okt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.ExcelFile('stopwords_ko_20191105.xlsx').parse('STOPWORDS').STOPWORDS\n",
    "stopword_replaced_voca_list = remove_stopwords(raw_noun_voca_list, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_custom = ['우리', '최근']\n",
    "stopword_replaced_voca_list = remove_stopwords(stopword_replaced_voca_list, stopwords_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_most_common_words(stopword_replaced_voca_list, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "common_words_path = 'data/sample_newspaper_common_word.csv'\n",
    "save_most_common_words(stopword_replaced_voca_list, 200, common_words_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_noun_sentence_list = make_sentence_list_from_voca_list(stopword_replaced_voca_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'data/sample_newspaper_modified.csv'\n",
    "output_list_name = '기사_명사'\n",
    "\n",
    "raw_df[output_list_name] = raw_noun_sentence_list\n",
    "raw_df.to_csv(output_path, sep=',', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}